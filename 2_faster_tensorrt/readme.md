# Faster tensorrt

## 前言

使用之前你应该已经了解trt的构建和推理流程，所以此处不再涉及基础使用。你应该修改的最少有
```
1. CMakeLists.txt中的cuda、cudnn、trnsorrt环境路径
2. main.cpp中的测试推理图片/视频的路径、trt二进制文件路径，推理类别等
3. 预处理和后处理也要根据实际使用模型修改，本文代码以yolox为例
```

原始的TensorRT_Pro有十分优秀的性能，并且接口的设计也很巧妙。但是我在复现和使用的时候发现部分可能不太适用于我当前使用的机器人。
1. 它的加速是在将需要推理的所有图像全部commit, 然后它内部每个batch的加载和推理, 加速主要集中在batch数据的预处理和推理异步进行，也就是prefetch的思想。但是在单目机器人上往往是视频流输入，此时是一般是不能输入batch数据的，所以此时实际上是不会比直接推理快多少。

2. 但图像commit没有任务队列管理

3. 它用的是自写的CUDA NMS，但是实际上TensorRT8上有一些官方的NMS插件，可以替换。两者的实际效果带测试。



## 文件说明

我在大多数地方都已经加了中文注释，应该能够容易看懂。当然注释可能也会有写错或者理解错误啥的，还是需要有自己的思考的，也欢迎一起交流。

### include

1. `tools.hpp`: 一些工具函数 包括log日志打印，CUDA检查，输出文件保存读取等定义并直接实现
2. `memory_tensor.hpp`: 定义`MixMemory`实现内存和显存的申请和释放；定义`Tensor`实现张量的管理、扩容、拷贝等
3. `monopoly_accocator.hpp`: 定义内存独占管理分配器，最终实现预处理和推理并行的重要工具
4. `infer_base.hpp`: 定义trt引擎管理类和异步安全推理类
5. `yolo.hpp`: 定义yolo的推理

### src

1. `cuda_kernel.cu`: cuda核函数的实现，预处理和后处理相关的cuda加速代码
2. `memory_tensor.cpp`: `MixMemory`和`Tensor`的实现
3. `infer_base.cpp`: trt引擎管理类和异步安全推理类的实现
4. `yolo.cpp`: yolo推理的实现
5. `main.cpp`: 主函数

### src/eval
这是一个评估代码，可以测试相关数据集(coco格式)使用trt推理的map。



## 使用教程

### 模型转换


### 模型推理


### 模型测评

## 优化

- [ ] 预分配内存池，避免在推理阶段重复分配
- [ ] gpu内存异步操作内核融合，使用一个gpu内核实现运算符组合，减少数据传输和内核启动延迟
- [ ] 向量化全局内存访问，提高内存访问效率
- [ ] 一个tensorrt的engine可以创建多个context，实现多线程调用。只占用一个engine显存的大小，同时供多个推理运算

