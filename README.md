# Faster Deployment

>本文都是能力和想法都十分有限的我的一家之言，确实很多情况没有想到，欢迎讨论！

本仓库主要是深度学习模型的TensorRT、ncnn部署，有较好的接口便捷性和推理延迟。均以目标检测部署为例。rknn的部署也开源了可见[zzx_rknn](https://github.com/0zzx0/zzx_rknn)
> 需要说明的是，不同于其他benchmark，本项目因为主要适用于机器人，所以在测试是一般设置`batch=1`，采用一张接着一张图片输入或者视频输入的方式进行测试，模拟单目机器人的实际情况。

- 1_trt_learn: 主要是tensorrt的基础操作，包括模型转换、推理、构建插件以及一些运行和优化的demo。
- 2_faster_tensorrt: 主要是tensorrt的封装和优化
- 3_faster_ncnn: 参考`2_faster_tensort`封装ncnn推理过程

## 致谢
首先需要感谢手写ai团队开源的[TensorRT_Pro](https://github.com/shouxieai/tensorRT_Pro)，让我受益良多，本仓库中tensorrt的代码也均是在其基础上进行优化，以及按照该仓库的整体思路优化ncnn的推理。

<!-- 该仓库的部分优点：
1. 接口简单清晰
2. 预处理和后处理自写CUDA加速
3. batch可根据实际数据动态调整(前提是trtmodel转换中设置动态batch)
4. 写了内存和数据的管理类，无需手动操作，并且可以实现内存复用，无需反复申请。
5. 预处理和推理同时进行
6. 生产者消费者模式，合理好用。 -->



## 问题

我们首先应该思考在实际机器人的视频流推理上需要的是什么？

明确前提：我们此时的模型已经充分优化过了，包括剪枝、量化之类的操作之后或者后端推理器已经对模型算子进行了自动或手动的融合、量化等操作。**简而言之，可以认为单幅图像的纯inference时间是不可能再缩短了。**

<font size=5 >**高吞吐和低延迟！！！**</font>

> 本文中两个词的意义 :   <br>
> **延迟**：图片从诞生到推理完成需要的时间。 <br>
> **吞吐**：相等时间内处理图片的数量。 <br>


我们的目标肯定是吞吐量特别大，同时延迟超级小，在一定条件下，其实这两个是有互斥的意思的。但是另一方面，其实这也是表明程序性能两种方式，吞吐量代表并行能力，延迟代表串行效果。

之所以说有互斥的意思是因为，为了提高吞吐量，往往会先缓存一些数据，成为一个batch进行推理(也是tensorrt_pro的策略)。也就是相同时间内，可以实现更多图像的推理。BUT，这对于每一帧图像来说，它从输入到输出的延迟就会提高了！

另外采用多线程或者线程池推理，对机器人处理图像带来的问题是：在输入图像帧率很高的时候，无法保证图像按照输入顺序输出，这对于机器人这种需要根据目标前后运动状态进行决策的智能体来说是有问题的，可能会导致误判。

## 总结

目前我认为，推理的多线程可能不适合用于搭载单目摄像头机器人，但如果机器人搭载多摄像头，则其效果理论上要优于单线程，但是如果检测任务一致的话明多batch推理更加适合，可以酌情考虑。

那本仓库要做的是什么？

1. 首先我暂时抛弃了多batch，因为我当前使用的机器人不需要多输入。
2. 设置可调长度的任务队列(超过则阻塞)，可以根据模型预处理和推理耗时手动调整，保证最优的吞吐和延迟。
3. 尽量在可加速的硬件上执行预处理和后处理。



## other
https://github.com/PaddlePaddle/FastDeploy/blob/develop/tutorials/multi_thread/README_CN.md

